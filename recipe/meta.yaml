{% set version = "0.4.25" %}
{% set build = 0 %}

{% if cuda_compiler_version != "None" %}
{% set build = build + 200 %}
{% endif %}


package:
  name: jaxlib
  version: {{ version }}

source:
  # only pull sources after upstream PyPI release...
  url: https://github.com/google/jax/archive/jaxlib-v{{ version }}.tar.gz
  sha256: fc1197c401924942eb14185a61688d0c476e3e81ff71f9dc95e620b57c06eec8
  patches:
    - patches/0001-Allow-for-custom-CUDA-build.patch
    - patches/0002-xla-Support-third-party-build-of-boringssl.patch
    - patches/0003-xla-Fix-abseil-headers.patch

build:
  number: {{ build }}
  skip: true  # [win or py<39]
  skip: true  # [cuda_compiler != "None" and aarch64]
  skip: true  # [cuda_compiler_version == "11.2"]
  string: cuda{{ cuda_compiler_version | replace('.', '') }}py{{ CONDA_PY }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [cuda_compiler_version != "None"]
  string: cpu_py{{ CONDA_PY }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [cuda_compiler_version == "None"]

requirements:
  build:
    - {{ compiler('c') }}
    - {{ stdlib("c") }}
    - {{ compiler('cxx') }}
    - {{ compiler('cuda') }}                 # [cuda_compiler_version != "None"]
    # For some reason we get 2.28. pin to 2.17
    - python                                 # [build_platform != target_platform]
    - cross-python_{{ target_platform }}     # [build_platform != target_platform]
    - numpy                                  # [build_platform != target_platform]
    - unzip
    - rsync  # [(cuda_compiler_version or "").startswith("12")]
    # Keep bazel listed twice here to help the migrators track dependencies
    - bazel
    - bazel >=5.1.1,<6
    - bazel-toolchain >=0.1.10
    # need protoc
    - libprotobuf
    # needs protoc-gen-grpc
    - libgrpc
    # needs flatc
    - flatbuffers <2.0.6
    # list libabseil here to ensure pinning correctly
    - libabseil
  host:
    - cudnn             # [cuda_compiler_version != "None"]
    - nccl              # [cuda_compiler_version != "None"]
    - cuda-cupti-dev    # [(cuda_compiler_version or "").startswith("12")]
    - cuda-cudart-dev   # [(cuda_compiler_version or "").startswith("12")]
    - cuda-nvml-dev     # [(cuda_compiler_version or "").startswith("12")]
    - cuda-nvtx-dev     # [(cuda_compiler_version or "").startswith("12")]
    - libcublas-dev     # [(cuda_compiler_version or "").startswith("12")]
    - libcusolver-dev   # [(cuda_compiler_version or "").startswith("12")]
    - libcurand-dev     # [(cuda_compiler_version or "").startswith("12")]
    - libcufft-dev      # [(cuda_compiler_version or "").startswith("12")]
    - libcusparse-dev   # [(cuda_compiler_version or "").startswith("12")]
    - python
    - pip
    - numpy
    - wheel
    - cuda-version {{ cuda_compiler_version }}  # [cuda_compiler_version != "None"]
    # avoid not being able to pass `-C=--build-option=--python-tag=cp<x>` due to
    # https://github.com/pypa/build/issues/202, which is being used by jaxlib in
    # https://github.com/google/jax/blame/jaxlib-v0.4.15/jaxlib/tools/build_wheel.py
    - python-build <1
    # list libabseil here to ensure pinning correctly
    - libabseil
    - flatbuffers <2.0.6
    - libgrpc
    - openssl
    - zlib
  run:
    - python
    - scipy >=1.9
    - ml_dtypes >=0.2.0
    - __cuda  # [cuda_compiler_version != "None"]
    - cuda-nvcc  # [(cuda_compiler_version or "").startswith("12")]
    # Workaround for https://github.com/conda-forge/cuda-cupti-feedstock/issues/14
    - cuda-cupti >=12.0.90,<13.0a0  # [(cuda_compiler_version or "").startswith("12")]
  run_constrained:
    - jax >={{ version }}

test:
  files:
    - test_jaxlib.py
  requires:
    - pip
  commands:
    - pip check
    - python test_jaxlib.py
  imports:
    - jaxlib
    # Only this import actually triggers the load of XLA
    - jaxlib.xla_client

about:
  home: http://github.com/google/jax
  license: Apache-2.0
  license_file: LICENSE
  summary: 'Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more'
  dev_url: http://github.com/google/jax

extra:
  recipe-maintainers:
    - ericmjl
    - xhochy
    - ngam
